Helping a Retail Business Understand their Customers Better - A Customer Segmentation Using PCA and KMeans 
1. Introduction
I am entry level data scientist for a data consultancy agency and I have been tasked with carrying out a customer segmentation project for a retail store whose customer data was provided. Understanding customer behavior is essential for effective marketing, customer retention, and revenue growth. This project develops a full machine-learning pipeline for customer segmentation based on demographic and purchase behavioral features using:
•	Categorical encoders
•	StandardScaler
•	PCA (1D dimensionality reduction)
•	KMeans clustering
•	Pickle-based model persistence
The goal was to produce a segmentation model that is reusable, interpretable, and capable of handling real-world categorical and numeric data consistently.
2. Problem Statement
The retail business struggle to target customers effectively because raw customer data is high-dimensional, mixed-type (categorical + numeric), and not easily separable. Without proper segmentation, marketing campaigns have become broad, inefficient, and expensive.
This project addressed the problem by building a machine-learning pipeline that:
•	Converts mixed data types into a uniform, numeric representation
•	Reduces dimensional complexity using PCA
•	Uses clustering to discover natural patterns within the customer base
3. Project Objectives
•	Preprocess customer data using appropriate encoders for each categorical type
•	Normalize numeric variables using StandardScaler
•	Apply PCA (n_components=1) to reduce high-dimensional inputs into a single latent dimension
•	Train a KMeans clustering model on the PCA-transformed data
•	Save and load preprocessing objects and the trained model with pickle
•	Visualize and interpret clusters for both training and test datasets
4. Project Workflow and Methodology
4.1 Data Loading & Cleaning
•	Loaded dataset using pandas
•	Checked for nulls, inconsistencies, and duplicate values
•	Cleaned irregular values and standardized categorical formats
•	Changed fields to the appropriate data types and removed redundant fields
4.2 Exploratory Data Analysis (EDA)
•	Frequency analysis of categorical variables
•	Distribution analysis for numeric fields
•	Correlation matrix for numeric variables
•	Pairplot to visualise relationship between fields
•	Variance of the features
4.3 Feature Engineering
Different encoders were applied based on variable type:
•	OneHotEncoder → nominal categorical variables
•	OrdinalEncoder → ordered categories
•	LabelEncoder → specific single-field mappings
•	StandardScaler → numeric fields
Encoded arrays were merged using either:
•	pandas.concat (keeps feature names), and
•	np.hstack (clean, numeric matrix for PCA)
4.4 Dimensionality Reduction with PCA
•	Applied PCA with n_components = 1
•	Objective: compress information from many encoded columns into one representative latent dimension
•	Benefits:
1.	Eliminate noisy or redundant features
2.	Improve clustering stability
3.	Provide a single dimension for easy visualization
4.5 Model Building with KMeans
Two separate experiments were conducted:
Experiment 1 — PCA + KMeans
•	Fit KMeans on PCA output
•	Achieved meaningful clusters
•	Evaluated using silhouette score: Silhouette scores for PCA trained model was highest at 0.92 for 5 clusters 
Experiment 2 — Single Highest Variance Feature
•	Identified numeric variable with highest variance
•	Built alternative KMeans model
•	Compared cluster separation and silhouette score
•	PCA-based model performed more consistently
4.6 Model Saving & Loading (Pickle)
Saved:
•	Fitted PCA
•	Fitted KMeans
This allows the full pipeline to be reproduced in testing without refitting.
4.7 Model Testing
•	Loaded the saved preprocessing objects
•	Transformed test dataset using transform only
•	Applied PCA and KMeans to generate cluster labels
•	Visualized 1D PCA distribution and cluster stripplot

5. Results & Interpretation
5.1 PCA Output
•	Single latent feature captured majority of useful variance
•	Clear spread of customers along the PCA axis
5.2 Cluster Visualization
•	Clusters show varying density and separation
•	Some segments represent high-activity shoppers
•	Others represent more passive customers
•	Cluster plot enables quick business interpretation

6. Project Limitations
6.1 Data Limitations
•	Dataset contains limited numeric fields
•	PCA effectiveness depends on variance; some categorical encodings may dilute signal
•	Some categorical variables had low diversity → weak contribution to clustering
6.2 Modeling Limitations
•	PCA compresses information, but 1D PCA may underrepresent certain relationships
•	KMeans assumes spherical clusters—may not match real customer behavior
6.3 Technical Limitations
•	Manual encoding approaches risk misalignment during testing
•	LabelEncoder can disrupt categorical consistency if categories differ
•	Test dataset must strictly mirror training format

7. Areas for Improvement
7.1 Data Improvements
•	Add more behavioral features (frequency, total spend, recency, product categories)
•	Include temporal characteristics (lifetime activity, churn probability)
7.2 Modeling Enhancements
•	Use UMAP or t-SNE for non-linear dimensionality reduction
•	Explore alternative clustering techniques:
o	Gaussian Mixture Models
o	DBSCAN
o	Agglomerative clustering
o	HDBSCAN (very strong for mixed-density data)
7.3 Pipeline Enhancements
•	Replace LabelEncoder with more robust encoders
•	Move everything into a single production pipeline
•	Add automated test scripts to validate model performance
•	Standardize visualization outputs for reporting

8. Tools and Technologies
•	Python
•	pandas, NumPy
•	scikit-learn
•	PCA, KMeans
•	Pickle
•	Matplotlib, Seaborn
•	Github desktop
9. Model Testing Workflow
The workflow consists of:
9.1 Test Data Loading and Wrangling
The notebook defines a reusable wrangle() function that:
•	Loads the raw test CSV
•	Drops null values
•	Performs basic cleaning
•	Returns a usable dataframe
def wrangle(filepath):
    # read csv file
    data = pd.read_csv(filepath)
    # load csv file into a data frame
    df = pd.DataFrame(data)
    # drop null values
    df.dropna(inplace = True)
    # remove duplicate files
    df.drop_duplicates(inplace = True)
    # drop redundant columns
    df.drop(columns = ['id','Unnamed: 0', 'Segmentation', 'Var_1'], inplace = True)
    #df['id'] = df['id'].astype(object)
    df['Work_Experience'] = df['Work_Experience'].astype(int)
    df['Family_Size'] =df['Family_Size'].astype(int)
    return df
This is a clean and efficient approach. Wrangling is consistent between training and testing.

9.2 Test Dataset Inspection
df_test = wrangle(filepath)
df_test.head()
df_test.describe()
df_test.info()
df_test.head()

9.3 Loading Pretrained Components with Pickle
The model testing: loaded previously saved objects:
•	PCA model (n_components = 1)
•	Trained KMeans
9.4 Applying the Same Preprocessing to Test Data
1.	The same encoding strategy
2.	The same scaling strategy
3.	The same PCA model (transform only)
4.	The same KMeans model
9.5 Dimensionality Reduction on Test Data
Used saved PCA for dimensionality reduction
This reduces all features into a single PCA component, maintaining full consistency with your training pipeline.
9.6 Generating Cluster Predictions
The model predicts the test clusters, creating cluster labels for every test customer.

9.7 Visualizing the Test Clusters
You use seaborn/plotly to visualize:
•	PCA 1D distribution
•	Cluster stripplots

10. Conclusion
This project successfully demonstrated the complete workflow for customer segmentation using PCA and KMeans. The final model can consistently encode, scale, reduce, and cluster real-world customer datasets. While limitations exist due to data richness and mixed-data structure, the pipeline provides a strong foundation for business-focused segmentation and future machine-learning enhancement.

